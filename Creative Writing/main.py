import os
import json
from text_create_engine import TreeOfThought, ChainOfThought, TextEvaluator

def print_separator(title: str):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "="*60)
    print(f" {title} ")
    print("="*60)

def print_evaluation_results(evaluation: dict):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ ({evaluation['method']}):")
    print(f"   åˆ›æ„æ€§: {evaluation['creativity']:.1f}/5.0")
    print(f"   è¿è´¯æ€§: {evaluation['coherence']:.1f}/5.0")
    print(f"   æ–‡å­¦ä»·å€¼: {evaluation['literary_value']:.1f}/5.0")
    print(f"   æƒ…æ„Ÿå…±é¸£: {evaluation['emotional_resonance']:.1f}/5.0")
    print(f"   å®Œæ•´æ€§: {evaluation['completeness']:.1f}/5.0")
    print(f"   æ€»åˆ†: {evaluation['total_score']:.1f}/25.0")
    print(f"   è¯„ä»·: {evaluation['evaluation']}")

def main():
    """ä¸»å‡½æ•°"""
    print_separator("æ ‘çŠ¶æ€ç»´é“¾ vs ä¼ ç»Ÿæ€ç»´é“¾åˆ›æ„å†™ä½œå¯¹æ¯”å®éªŒ")
    print("ä½¿ç”¨OpenAI 0.29.0åº“è°ƒç”¨DeepSeekå¤§æ¨¡å‹")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = "Your-API-Key"
    base_url = "https://api.deepseek.com/v1"
    
    if not api_key:
        print("âŒ é”™è¯¯: è¯·è®¾ç½®ä½ çš„ DEEPSEEK_API_KEY")
        return
    
    print(f"ğŸ”‘ ä½¿ç”¨API: {base_url}")
    
    # åˆ›æ„å†™ä½œæç¤ºè¯
    prompts = [
        "å†™ä¸€ä¸ªå…³äºæ—¶é—´æ—…è¡Œè€…åœ¨å¤ä»£é‡åˆ°ç°ä»£ç§‘æŠ€çš„å¥‡å¹»æ•…äº‹",
        "æè¿°ä¸€ä¸ªèƒ½å¤Ÿå¬æ‡‚åŠ¨ç‰©è¯­è¨€çš„å°å¥³å­©çš„å†’é™©ç»å†",
        "åˆ›ä½œä¸€ä¸ªå…³äºæœ€åä¸€æœ¬å®ä½“ä¹¦çš„ç§‘å¹»æ•…äº‹",
        "å†™ä¸€ä¸ªå…³äºä¼šåšæ¢¦çš„æœºå™¨äººçš„æ¸©é¦¨æ•…äº‹",
        "ä»¥é›¨å¤œã€åœ£å™¨ã€æµæµªçŒ«ä¸ºå…³é”®è¯å†™ä¸€ç¯‡æ–‡ç« "
    ]
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸ”§ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
    tot = TreeOfThought(api_key, base_url)
    cot = ChainOfThought(api_key, base_url)
    evaluator = TextEvaluator(api_key, base_url)
    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    results = []
    
    for i, prompt in enumerate(prompts, 1):
        print_separator(f"å®éªŒ {i}: {prompt}")
        
        try:
            # ç”Ÿæˆä¼ ç»Ÿæ€ç»´é“¾ç»“æœ
            print("ğŸ”— ç”Ÿæˆä¼ ç»Ÿæ€ç»´é“¾ç»“æœ...")
            cot_result = cot.generate_cot_response(prompt)
            print(f"âœ… CoTç»“æœç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(cot_result)} å­—ç¬¦)")
            
            # ç”Ÿæˆæ ‘çŠ¶æ€ç»´é“¾ç»“æœ
            print("\nğŸŒ³ ç”Ÿæˆæ ‘çŠ¶æ€ç»´é“¾ç»“æœ...")
            tot_state = tot.tree_of_thought_search(prompt)
            tot_result = tot_state.content
            print(f"âœ… ToTç»“æœç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(tot_result)} å­—ç¬¦)")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„å†…å®¹
            print_separator("ç”Ÿæˆå†…å®¹å¯¹æ¯”")
            print("ğŸ“ ä¼ ç»Ÿæ€ç»´é“¾ (CoT) ç»“æœ:")
            print("-" * 40)
            print(cot_result)
            
            print("\nğŸ“ æ ‘çŠ¶æ€ç»´é“¾ (ToT) ç»“æœ:")
            print("-" * 40)
            print(tot_result)
            
            # è¯„ä¼°å’Œå¯¹æ¯”
            print_separator("è¯„ä¼°ä¸å¯¹æ¯”")
            print("ğŸ¤– è°ƒç”¨AIè¯„ä¼°å‘˜è¿›è¡Œå¤šç»´åº¦è¯„åˆ†...")
            comparison = evaluator.compare_methods(prompt, cot_result, tot_result)
            
            # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
            print_evaluation_results(comparison['cot_evaluation'])
            print()
            print_evaluation_results(comparison['tot_evaluation'])
            
            # æ˜¾ç¤ºæ”¹è¿›æƒ…å†µ
            print(f"\nğŸ“ˆ æ”¹è¿›æƒ…å†µ:")
            improvements = comparison['improvements']
            for key, value in improvements.items():
                if key == 'total_score':
                    continue
                key_name = {
                    'creativity': 'åˆ›æ„æ€§',
                    'coherence': 'è¿è´¯æ€§', 
                    'literary_value': 'æ–‡å­¦ä»·å€¼',
                    'emotional_resonance': 'æƒ…æ„Ÿå…±é¸£',
                    'completeness': 'å®Œæ•´æ€§'
                }.get(key, key)
                
                if value > 0:
                    print(f"   {key_name}: +{value:.1f} â¬†ï¸")
                elif value < 0:
                    print(f"   {key_name}: {value:.1f} â¬‡ï¸")
                else:
                    print(f"   {key_name}: {value:.1f} â¡ï¸")
            
            total_improvement = improvements['total_score']
            if total_improvement > 0:
                print(f"   æ€»åˆ†æå‡: +{total_improvement:.1f} åˆ† ğŸ‰")
                improvement_percentage = (total_improvement / comparison['cot_evaluation']['total_score']) * 100
                print(f"   æå‡å¹…åº¦: {improvement_percentage:.1f}%")
            else:
                print(f"   æ€»åˆ†å˜åŒ–: {total_improvement:.1f} åˆ†")
            
            # ä¿å­˜ç»“æœ
            result = {
                'experiment_id': i,
                'prompt': prompt,
                'cot_result': cot_result,
                'tot_result': tot_result,
                'comparison': comparison,
                'timestamp': str(os.popen('date').read().strip())
            }
            results.append(result)
            
            print(f"\n{'ğŸ† æ ‘çŠ¶æ€ç»´é“¾è·èƒœ!' if comparison['tot_better'] else 'ğŸ¤” ä¼ ç»Ÿæ€ç»´é“¾è¡¨ç°æ›´å¥½'}")
            
        except Exception as e:
            print(f"âŒ å®éªŒ {i} æ‰§è¡Œå¤±è´¥: {e}")
            continue
    
    # æ€»ç»“æŠ¥å‘Š
    print_separator("å®éªŒæ€»ç»“æŠ¥å‘Š")
    
    if results:
        tot_wins = sum(1 for r in results if r['comparison']['tot_better'])
        total_experiments = len(results)
        
        print(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
        print(f"   æ€»å®éªŒæ•°: {total_experiments}")
        print(f"   æ ‘çŠ¶æ€ç»´é“¾è·èƒœ: {tot_wins} æ¬¡")
        print(f"   ä¼ ç»Ÿæ€ç»´é“¾è·èƒœ: {total_experiments - tot_wins} æ¬¡")
        print(f"   æ ‘çŠ¶æ€ç»´é“¾èƒœç‡: {tot_wins/total_experiments*100:.1f}%")
        
        # è®¡ç®—å¹³å‡æ”¹è¿›
        avg_improvements = {}
        for key in ['creativity', 'coherence', 'literary_value', 'emotional_resonance', 'completeness', 'total_score']:
            avg_improvement = sum(r['comparison']['improvements'][key] for r in results) / len(results)
            avg_improvements[key] = avg_improvement
        
        print(f"\nğŸ“ˆ å¹³å‡æ”¹è¿›æƒ…å†µ:")
        for key, value in avg_improvements.items():
            if key == 'total_score':
                continue
            key_name = {
                'creativity': 'åˆ›æ„æ€§',
                'coherence': 'è¿è´¯æ€§',
                'literary_value': 'æ–‡å­¦ä»·å€¼', 
                'emotional_resonance': 'æƒ…æ„Ÿå…±é¸£',
                'completeness': 'å®Œæ•´æ€§'
            }.get(key, key)
            print(f"   {key_name}: {value:+.2f}")
        
        print(f"   æ€»åˆ†å¹³å‡æå‡: {avg_improvements['total_score']:+.2f}")
        
        # è®¡ç®—å¹³å‡æå‡ç™¾åˆ†æ¯”
        avg_cot_score = sum(r['comparison']['cot_evaluation']['total_score'] for r in results) / len(results)
        if avg_cot_score > 0:
            avg_improvement_percentage = (avg_improvements['total_score'] / avg_cot_score) * 100
            print(f"   å¹³å‡æå‡å¹…åº¦: {avg_improvement_percentage:+.1f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        output_data = {
            'experiment_summary': {
                'total_experiments': total_experiments,
                'tot_wins': tot_wins,
                'cot_wins': total_experiments - tot_wins,
                'tot_win_rate': tot_wins/total_experiments,
                'average_improvements': avg_improvements,
                'openai_version': '0.29.0',
                'model': 'deepseek-chat'
            },
            'detailed_results': results
        }
        
        with open('experiment_results.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° experiment_results.json")
        
        # ç»“è®º
        print_separator("ç»“è®º")
        if avg_improvements['total_score'] > 0:
            print("ğŸ‰ å®éªŒç»“æœè¡¨æ˜ï¼šæ ‘çŠ¶æ€ç»´é“¾åœ¨åˆ›æ„å†™ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾ï¼")
            print(f"   å¹³å‡æ€»åˆ†æå‡: {avg_improvements['total_score']:.2f} åˆ†")
            if avg_cot_score > 0:
                print(f"   å¹³å‡æå‡å¹…åº¦: {avg_improvement_percentage:+.1f}%")
            print("   ä¸»è¦ä¼˜åŠ¿ä½“ç°åœ¨å¤šè·¯å¾„æ¢ç´¢å’Œæœ€ä¼˜é€‰æ‹©æœºåˆ¶ä¸Šã€‚")
            print("\nğŸ” ä¼˜åŠ¿åˆ†æ:")
            
            best_improvements = sorted(avg_improvements.items(), key=lambda x: x[1], reverse=True)
            for key, value in best_improvements[:3]:
                if key != 'total_score' and value > 0:
                    key_name = {
                        'creativity': 'åˆ›æ„æ€§',
                        'coherence': 'è¿è´¯æ€§',
                        'literary_value': 'æ–‡å­¦ä»·å€¼', 
                        'emotional_resonance': 'æƒ…æ„Ÿå…±é¸£',
                        'completeness': 'å®Œæ•´æ€§'
                    }.get(key, key)
                    print(f"   - {key_name}æå‡æœ€æ˜¾è‘—: +{value:.2f}åˆ†")
        else:
            print("ğŸ¤” å®éªŒç»“æœæ˜¾ç¤ºä¸¤ç§æ–¹æ³•è¡¨ç°ç›¸è¿‘ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ ·æœ¬æˆ–è°ƒæ•´å‚æ•°ã€‚")
            print("   å»ºè®®å¢åŠ å®éªŒæ ·æœ¬æ•°é‡æˆ–è°ƒæ•´æ ‘æœç´¢æ·±åº¦å‚æ•°ã€‚")
        
        print(f"\nğŸ“‹ æŠ€æœ¯ç»†èŠ‚:")
        print(f"   - OpenAIåº“ç‰ˆæœ¬: 0.29.0")
        print(f"   - ä½¿ç”¨æ¨¡å‹: deepseek-chat")
        print(f"   - æ ‘æœç´¢æ·±åº¦: {tot.max_depth}")
        print(f"   - æ¯æ­¥å€™é€‰æ•°: {tot.num_thoughts_per_step}")
        
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸå®Œæˆçš„å®éªŒï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
        print("ğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("   1. æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ DEEPSEEK_API_KEY æ˜¯å¦æ­£ç¡®")
        print("   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("   3. éªŒè¯DeepSeek APIé¢åº¦æ˜¯å¦å……è¶³")

if __name__ == "__main__":
    main()